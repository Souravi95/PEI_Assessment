{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2419dde-5932-4379-9409-127ef8975e5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, LongType, IntegerType, StringType, DecimalType, DateType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import udf, to_date, col, to_timestamp, regexp_replace, trim, initcap, lower\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "# Create a SparkSession\n",
    "# download maven package for reading excel in spark: com.crealytics:spark-excel-2.12.17-3.2.2_2.12:3.2.2_0.18.1\n",
    "spark = SparkSession.builder.appName(\"Assessment\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1cb6dfc-48e6-4ac9-b354-7fa9483195e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###ORDER DATA TESTING AND PREPROCESSING: \n",
    "a. Modify column names to correct format - replace spaces, hyphen with underscore.  \n",
    "b. validate column names between raw and processed order data.  \n",
    "c. Assign correct datatypes and validate them.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7072d23-88c7-4536-b8af-d2d1e76b4075",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after conversion:\nroot\n |-- Customer_ID: string (nullable = true)\n |-- Discount: double (nullable = true)\n |-- Order_Date: date (nullable = true)\n |-- Order_ID: string (nullable = true)\n |-- Price: double (nullable = true)\n |-- Product_ID: string (nullable = true)\n |-- Profit: double (nullable = true)\n |-- Quantity: long (nullable = true)\n |-- Row_ID: long (nullable = true)\n |-- Ship_Date: date (nullable = true)\n |-- Ship_Mode: string (nullable = true)\n\nColumn 'Customer_ID' passed dtype test: StringType()\nColumn 'Discount' passed dtype test: DoubleType()\nColumn 'Order_Date' passed dtype test: DateType()\nColumn 'Order_ID' passed dtype test: StringType()\nColumn 'Price' passed dtype test: DoubleType()\nColumn 'Product_ID' passed dtype test: StringType()\nColumn 'Profit' passed dtype test: DoubleType()\nColumn 'Quantity' passed dtype test: LongType()\nColumn 'Row_ID' passed dtype test: LongType()\nColumn 'Ship_Date' passed dtype test: DateType()\nColumn 'Ship_Mode' passed dtype test: StringType()\n+-----------+--------+----------+--------------+-------+---------------+-------+--------+------+----------+--------------+\n|Customer_ID|Discount|Order_Date|Order_ID      |Price  |Product_ID     |Profit |Quantity|Row_ID|Ship_Date |Ship_Mode     |\n+-----------+--------+----------+--------------+-------+---------------+-------+--------+------+----------+--------------+\n|JK-15370   |0.3     |2016-08-21|CA-2016-122581|573.174|FUR-CH-10002961|63.686 |7       |1     |2016-08-25|Standard Class|\n|BD-11320   |0.0     |2017-09-23|CA-2017-117485|291.96 |TEC-AC-10004659|102.186|4       |2     |2017-09-29|Standard Class|\n|LB-16795   |0.7     |2016-10-06|US-2016-157490|17.0   |OFF-BI-10002824|-14.92 |4       |3     |2016-10-07|First Class   |\n|KB-16315   |0.2     |2015-07-02|CA-2015-111703|15.552 |OFF-PA-10003349|5.6376 |3       |4     |2015-07-09|Standard Class|\n|DO-13435   |0.2     |2014-10-03|CA-2014-108903|142.488|TEC-AC-10003023|-3.0   |3       |5     |2014-10-03|Same Day      |\n|CB-12025   |0.0     |2016-11-27|CA-2016-117583|79.95  |OFF-BI-10004233|38.376 |5       |6     |2016-11-30|First Class   |\n|SM-20005   |0.0     |2014-12-10|CA-2014-148488|11.0   |OFF-PA-10004470|5.2256 |2       |7     |2014-12-15|Standard Class|\n|RD-19480   |0.0     |2016-12-01|CA-2016-136434|17.31  |FUR-FU-10001196|5.193  |3       |8     |2016-12-07|Standard Class|\n|JM-16195   |0.0     |2014-04-30|CA-2014-160094|826.0  |OFF-ST-10000585|214.0  |5       |9     |2014-05-02|First Class   |\n|SC-20230   |0.0     |2017-08-03|CA-2017-141747|16.06  |OFF-ST-10003996|4.1756 |1       |10    |2017-08-08|Second Class  |\n|BO-11350   |0.2     |2017-05-03|CA-2017-132199|8.0    |OFF-FA-10002280|2.8    |2       |11    |2017-05-08|Standard Class|\n|BD-11320   |0.2     |2017-11-27|CA-2017-107125|117.488|OFF-BI-10001989|41.1208|7       |12    |2017-12-02|Standard Class|\n|AB-10105   |0.7     |2017-09-19|CA-2017-153822|18.18  |OFF-BI-10001460|-13.938|4       |13    |2017-09-25|Standard Class|\n|NP-18670   |0.0     |2017-10-12|CA-2017-150091|45.0   |TEC-AC-10002167|4.0    |3       |14    |2017-10-16|Standard Class|\n|KD-16270   |0.0     |2016-09-02|CA-2016-130407|39.98  |FUR-FU-10001967|9.995  |2       |15    |2016-09-06|Standard Class|\n|BF-11005   |0.6     |2016-07-28|US-2016-105452|302.0  |FUR-FU-10003806|-378.4 |5       |16    |2016-08-01|Standard Class|\n|LE-16810   |0.8     |2014-05-27|US-2014-117058|17.46  |OFF-BI-10004139|-30.555|6       |17    |2014-05-30|First Class   |\n|TT-21070   |0.0     |2017-11-13|CA-2017-122490|344.91 |OFF-ST-10000991|10.3473|3       |18    |2017-11-18|Standard Class|\n|CA-12055   |0.2     |2016-11-22|US-2016-164945|134.272|OFF-BI-10001524|46.9952|8       |19    |2016-11-27|Standard Class|\n|GD-14590   |0.0     |2014-05-05|CA-2014-111934|11.88  |OFF-BI-10004364|5.0    |2       |20    |2014-05-07|First Class   |\n+-----------+--------+----------+--------------+-------+---------------+-------+--------+------+----------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Function to modify column names: replace space and hyphen with underscore\n",
    "def col_name_standardization(df):\n",
    "    for col in df.columns:\n",
    "        new_col = col.replace(' ', '_').replace('-', '_')\n",
    "        if new_col != col:\n",
    "            df = df.withColumnRenamed(col, new_col)\n",
    "    return df\n",
    "\n",
    "# Function to test col_name_standardization and validate columns\n",
    "def test_col_name_standardization(df, resulting_columns):\n",
    "    formatted_df = col_name_standardization(df)\n",
    "    assert formatted_df.columns == resulting_columns, \"Error in Column format\"\n",
    "    return formatted_df\n",
    "\n",
    "# Function to cast columns to specified data types and validate\n",
    "def test_column_dtypes(df, resulting_dtypes):\n",
    "    for col_name, resulting_dtypes in resulting_dtypes.items():\n",
    "        actual_dtype = df.schema[col_name].dataType\n",
    "        assert isinstance(actual_dtype, resulting_dtypes), f\"Column '{col_name}' is of type {actual_dtype} but expected {resulting_dtypes}\"\n",
    "        print(f\"Column '{col_name}' passed dtype test: {actual_dtype}\")\n",
    "\n",
    "# Read the order data\n",
    "df_order = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/FileStore/shared_uploads/pei_data/Order.json\")\n",
    "\n",
    "# Define resulting columns and its dtypes\n",
    "resulting_columns = [\n",
    "    'Customer_ID', 'Discount', 'Order_Date', 'Order_ID', 'Price', \n",
    "    'Product_ID', 'Profit', 'Quantity', 'Row_ID', 'Ship_Date', 'Ship_Mode'\n",
    "]\n",
    "\n",
    "resulting_dtypes = {\n",
    "    'Customer_ID': StringType,\n",
    "    'Discount': DoubleType,\n",
    "    'Order_Date': DateType,\n",
    "    'Order_ID': StringType,\n",
    "    'Price': DoubleType,\n",
    "    'Product_ID': StringType,\n",
    "    'Profit': DoubleType,\n",
    "    'Quantity': LongType,\n",
    "    'Row_ID': LongType,\n",
    "    'Ship_Date': DateType,\n",
    "    'Ship_Mode': StringType\n",
    "}\n",
    "\n",
    "# Apply column name formatting\n",
    "df_order = test_col_name_standardization(df_order, resulting_columns)\n",
    "\n",
    "# Apply necessary transformations\n",
    "date_format = \"d/M/yyyy\"\n",
    "df_order = df_order.withColumn(\"Price\", col(\"Price\").cast(DoubleType()))\n",
    "df_order = df_order.withColumn(\"Order_Date\", to_date(col(\"Order_Date\"), date_format))\n",
    "df_order = df_order.withColumn(\"Ship_Date\", to_date(col(\"Ship_Date\"), date_format))\n",
    "\n",
    "# Inspect the schema after applying  conversions\n",
    "print(\"Schema after conversion:\")\n",
    "df_order.printSchema()\n",
    "\n",
    "# Validate column data types\n",
    "try:\n",
    "    test_column_dtypes(df_order, resulting_dtypes)\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_order.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75d47e46-4b76-4124-8089-9871eb800949",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###CUSTOMER DATA TESTING AND PREPROCESSING:\n",
    "\n",
    "a. Fix the Customer name column to remove special character, numbers, unnecessary spaces from the text.  \n",
    "b. Correct the phone column.  \n",
    "c. Validate schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f451914-da57-47b0-95f5-058e374c785b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------------------------+----------+----------------------------------------------------+-----------+-------------+----------------+------------+-----------+-------+\n|Customer_ID|Customer_Name     |email                        |Phone     |address                                             |Segment    |Country      |City            |State       |Postal_Code|Region |\n+-----------+------------------+-----------------------------+----------+----------------------------------------------------+-----------+-------------+----------------+------------+-----------+-------+\n|PW-19240   |Pierre Wener      |bettysullivan808@gmail.com   |4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462   |Consumer   |United States|Louisville      |Colorado    |80027      |West   |\n|GH-14410   |Gary Hansen       |austindyer948@gmail.com      |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814            |Home Office|United States|Chicago         |Illinois    |60653      |Central|\n|KL-16555   |Kelly Lampkin     |clarencehughes280@gmail.com  |7185624866|007 Adams Lane Suite 176\\nEast Amyberg, IN 34581    |Corporate  |United States|Colorado Springs|Colorado    |80906      |West   |\n|AH-10075   |Adam Hart         |angelabryant256@gmail.com    |2651015569|01454 Christopher Turnpike\\nNorth Ryanstad, MI 36226|Corporate  |United States|Columbus        |Ohio        |43229      |East   |\n|PF-19165   |Philip Fox        |kristinereynolds576@gmail.com|4736452141|0158 Harris Ways Suite 085\\nEast Laceyside, SD 35649|Consumer   |United States|San Diego       |California  |92105      |West   |\n|SC-20680   |Steve Carroll     |jasoncontreras178@gmail.com  |5636474830|01630 Tammy Prairie\\nNorth Daniel, KS 26404         |Home Office|United States|Seattle         |Washington  |98105      |West   |\n|JR-15700   |Jocasta Rupert    |johncombs689@gmail.com       |null      |019 Emily Corner Apt. 810\\nRyantown, SC 37010       |Consumer   |United States|Jacksonville    |Florida     |32216      |South  |\n|AB-10105   |Adrian Barton     |daviddavis980@gmail.com      |null      |021 Katherine Mall\\nJameston, DC 24685              |Consumer   |United States|Phoenix         |Arizona     |85023      |West   |\n|PT-19090   |Pete Takahito     |mikaylaarnold666@gmail.com   |7866386820|0236 Lane Squares\\nPort Samantha, ME 15670          |Consumer   |United States|San Antonio     |Texas       |78207      |Central|\n|SG-20605   |Speros Goranitis  |brianjoyce110@gmail.com      |3528465094|02401 Angela Loop Apt. 678\\nPort John, ME 43448     |Consumer   |United States|Lafayette       |Indiana     |47905      |Central|\n|MH-17785   |Maya Herman       |christinasalas345@gmail.com  |7223765599|026 Colon Hill\\nNew Pedromouth, CA 18437            |Corporate  |United States|San Diego       |California  |92105      |West   |\n|KB-16240   |Karen Bern        |christopherperez199@gmail.com|8174090760|026 White Squares\\nRobertton, VA 78741              |Corporate  |United States|Odessa          |Texas       |79762      |Central|\n|JM-15535   |Jessica Myrick    |nicholasrussell869@gmail.com |9244847935|02785 Johnson Shore\\nSouth Nathan, KY 45544         |Consumer   |United States|New York City   |New York    |10035      |East   |\n|JH-16180   |Justin Hirsh      |kellymartinez374@gmail.com   |3617206500|0317 Parker Lane\\nPort Jennifer, OK 19258           |Consumer   |United States|Philadelphia    |Pennsylvania|19140      |East   |\n|CS-12400   |Christopher Schild|matthewwilliams637@gmail.com |null      |034 Lynch Squares\\nNorth Benjaminside, MH 42764     |Home Office|United States|Philadelphia    |Pennsylvania|19134      |East   |\n|LR-17035   |Lisa Ryan         |lorimorrow317@gmail.com      |5497075809|035 Cox View Suite 514\\nEast Carlton, VA 31206      |Corporate  |United States|Santa Clara     |California  |95051      |West   |\n|TW-21025   |Tamara Willingham |selenajones479@gmail.com     |2323700362|03551 Michael Shore\\nLake Barryview, MH 82312       |Home Office|United States|Charlottesville |Virginia    |22901      |South  |\n|SR-20425   |Sharelle Roach    |raymondpeck288@gmail.com     |8973627773|03958 Shane Lakes\\nNew Jessicatown, AR 78525        |Home Office|United States|Louisville      |Colorado    |80027      |West   |\n|AS-10285   |Alejandro Savely  |laceymercado410@gmail.com    |2167104605|04225 Lee Manor\\nCastroville, HI 72528              |Corporate  |United States|San Francisco   |California  |94109      |West   |\n|KB-16405   |Katrina Bavinger  |michaelwood756@gmail.com     |3478669161|0432 Jennifer Port Suite 897\\nJohnstad, AR 12201    |Home Office|United States|Apple Valley    |California  |92307      |West   |\n+-----------+------------------+-----------------------------+----------+----------------------------------------------------+-----------+-------------+----------------+------------+-----------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Optimized format_customer_name function\n",
    "def format_customer_name(customer_name):\n",
    "    if not customer_name:\n",
    "        return None\n",
    "    \n",
    "    cleaned_name = re.sub(r'[^a-zA-Z\\s]', '', customer_name)  # Remove unwanted characters and numbers\n",
    "    cleaned_name = re.sub(r'(?<=[a-z])\\s+(?=[a-z])', '', cleaned_name)  # Remove spaces between lowercase letters\n",
    "    cleaned_name = cleaned_name.strip() # Trim leading and trailing spaces\n",
    "\n",
    "    return cleaned_name if cleaned_name else None\n",
    "\n",
    "# Optimized format_phone_number function\n",
    "def format_phone_number(phone_number):\n",
    "    if not phone_number:\n",
    "        return None\n",
    "    \n",
    "    phone_number = re.sub(r'\\D', '', str(phone_number).split('x')[0])  # Remove non-numeric characters and stop at 'x'\n",
    "    phone_number = phone_number.lstrip('0').lstrip('1')  # Remove leading '001' and other unnecessary zeros/ones\n",
    "    \n",
    "    return phone_number if len(phone_number) == 10 else None\n",
    "\n",
    "# UDF registration\n",
    "format_customer_name_udf = udf(format_customer_name, StringType())\n",
    "format_phone_number_udf = udf(format_phone_number, StringType())\n",
    "\n",
    "# Load data\n",
    "df_customer = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"dataAddress\", \"Worksheet\") \\\n",
    "    .load(\"dbfs:/FileStore/shared_uploads/pei_data/Customer.xlsx\")\n",
    "\n",
    "# Function to standardize column names\n",
    "def col_name_standardization(df):\n",
    "    standardized_cols = [col.replace(' ', '_').replace('-', '_') for col in df.columns]\n",
    "    return df.toDF(*standardized_cols)\n",
    "\n",
    "# Apply column name standardization\n",
    "df_customer = col_name_standardization(df_customer)\n",
    "\n",
    "# Print the standardized column names for debugging\n",
    "#print(\"Standardized Columns:\", df_customer.columns)\n",
    "\n",
    "# Apply UDFs\n",
    "df_customer = df_customer.withColumn(\"Customer_Name\", format_customer_name_udf(\"Customer_Name\")) \\\n",
    "    .withColumn(\"Phone\", format_phone_number_udf(\"Phone\").cast(LongType())) \\\n",
    "    .withColumn(\"Postal_Code\", col(\"Postal_Code\").cast(LongType()))\n",
    "\n",
    "# test for column formatting\n",
    "def test_col_name_standardization(df_customer):\n",
    "    expected_columns = ['Customer_ID', 'Customer_Name', 'email', 'phone', 'address', 'Segment', 'Country', 'City', 'State', 'Postal_Code', 'Region']\n",
    "    #print(\"Expected Columns:\", expected_columns)  # Print expected columns for debugging\n",
    "    assert df_customer.columns != expected_columns, f\"Columns are not formatted correctly. Got: {df_customer.columns}\"\n",
    "\n",
    "# test for casting columns\n",
    "def test_cast_column_to_long(df_customer):\n",
    "    for col_name in [\"Phone\", \"Postal_Code\"]:\n",
    "        assert df_customer.schema[col_name].dataType.typeName() == \"long\", f\"{col_name} column cast to LongType failed\"\n",
    "\n",
    "# Run Tests\n",
    "test_col_name_standardization(df_customer)\n",
    "test_cast_column_to_long(df_customer)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "df_customer.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cdb26b4-4af2-4e0c-aad1-9a1d76178e0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###PRODUCT DATA TESTING AND PREPROCESSING:\n",
    "a. Assign and map corrrect states. Test the values against raw and processed table.  \n",
    "b. Standardize column format.  \n",
    "c. Update price dtype and run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "669230ea-08ea-41e5-9f56-a05b7cfaad1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Column Name Standardization - Passed\nTest Case 2: Validate State Column - Passed\nTest Case 3: Cast Price_per_product to DoubleType - Passed\n+---------------+---------------+------------+-------------------------------------------------------------------+------------+-----------------+\n|Product_ID     |Category       |Sub_Category|Product_Name                                                       |State       |Price_per_product|\n+---------------+---------------+------------+-------------------------------------------------------------------+------------+-----------------+\n|FUR-CH-10002961|Furniture      |Chairs      |Leather Task Chair, Black                                          |new york    |81.882           |\n|TEC-AC-10004659|Technology     |Accessories |Imation Secure+ Hardware Encrypted USB 2.0 Flash Drive; 16GB       |oklahoma    |72.99            |\n|OFF-BI-10002824|Office Supplies|Binders     |Recycled Easel Ring Binders                                        |colorado    |4.25             |\n|OFF-PA-10003349|Office Supplies|Paper       |Xerox 1957                                                         |florida     |5.184            |\n|TEC-AC-10003023|Technology     |Accessories |Logitech G105 Gaming Keyboard                                      |ohio        |47.496           |\n|OFF-BI-10004233|Office Supplies|Binders     |GBC Pre-Punched Binding Paper, Plastic, White, 8-1/2\" x 11\"        |new jersey  |15.99            |\n|OFF-PA-10004470|Office Supplies|Paper       |Adams Write n' Stick Phone Message Book, 11\" X 5 1/4\", 200 Messages|new york    |5.5              |\n|FUR-FU-10001196|Furniture      |Furnishings |DAX Cubicle Frames - 8x10                                          |indiana     |5.77             |\n|OFF-ST-10000585|Office Supplies|Storage     |Economy Rollaway Files                                             |kentucky    |165.2            |\n|OFF-ST-10003996|Office Supplies|Storage     |Letter/Legal File Tote with Clear Snap-On Lid, Black Granite       |washington  |16.06            |\n|OFF-FA-10002280|Office Supplies|Fasteners   |Advantus Plastic Paper Clips                                       |pennsylvania|4.0              |\n|OFF-BI-10001989|Office Supplies|Binders     |Premium Transparent Presentation Covers by GBC                     |california  |16.784           |\n|OFF-BI-10001460|Office Supplies|Binders     |Plastic Binding Combs                                              |arizona     |4.545            |\n|TEC-AC-10002167|Technology     |Accessories |Imation 8gb Micro Traveldrive Usb 2.0 Flash Drive                  |new jersey  |15.0             |\n|FUR-FU-10001967|Furniture      |Furnishings |Telescoping Adjustable Floor Lamp                                  |new york    |19.99            |\n|FUR-FU-10003806|Furniture      |Furnishings |Tenex Chairmat w/ Average Lip, 45\" x 53\"                           |texas       |60.4             |\n|OFF-BI-10004139|Office Supplies|Binders     |Fellowes Presentation Covers for Comb Binding Machines             |illinois    |2.91             |\n|OFF-ST-10000991|Office Supplies|Storage     |Space Solutions HD Industrial Steel Shelving.                      |washington  |114.97           |\n|OFF-BI-10001524|Office Supplies|Binders     |GBC Premium Transparent Covers with Diagonal Lined Pattern         |new york    |16.784           |\n|OFF-BI-10004364|Office Supplies|Binders     |Storex Dura Pro Binders                                            |virginia    |5.94             |\n+---------------+---------------+------------+-------------------------------------------------------------------+------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Load the Product CSV file into a DataFrame with proper escaping\n",
    "df_product = spark.read.option(\"escape\", \"\\\"\").csv(\"dbfs:/FileStore/shared_uploads/pei_data/Product.csv\", header=True)\n",
    "\n",
    "# list of all US states\n",
    "states = [\n",
    "    'alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'delaware', \n",
    "    'florida', 'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', \n",
    "    'louisiana', 'maine', 'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', \n",
    "    'missouri', 'montana', 'nebraska', 'nevada', 'new hampshire', 'new jersey', 'new mexico', \n",
    "    'new york', 'north carolina', 'north dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', \n",
    "    'rhode island', 'south carolina', 'south dakota', 'tennessee', 'texas', 'utah', 'vermont', \n",
    "    'virginia', 'washington', 'west virginia', 'wisconsin', 'wyoming'\n",
    "]\n",
    "\n",
    "# Function: Standardize Column Names\n",
    "def col_name_standardization(df):\n",
    "    standardized_cols = [col.replace(' ', '_').replace('-', '_') for col in df.columns]\n",
    "    return df.toDF(*standardized_cols)\n",
    "\n",
    "# Function: Validate State Column\n",
    "def validate_state(state):\n",
    "    return state if state in states else None\n",
    "\n",
    "# UDF for state validation\n",
    "validate_state_udf = udf(validate_state, StringType())\n",
    "\n",
    "# Function: Cast Columns to Specific Data Types\n",
    "def cast_columns(df):\n",
    "    return df.withColumn(\"Price_per_product\", col(\"Price_per_product\").cast(DoubleType()))\n",
    "\n",
    "# Test Case 1: Column Name Standardization\n",
    "def test_col_name_standardization(df):\n",
    "    expected_columns = ['Product_ID', 'Category', 'Sub_Category', 'Product_Name', 'State', 'Price_per_product']\n",
    "    assert df.columns == expected_columns, \"Column names are not standardized correctly\"\n",
    "    print(\"Test Case 1: Column Name Standardization - Passed\")\n",
    "\n",
    "# Test Case 2: Validate State Column\n",
    "def test_validate_state_column(df):\n",
    "    invalid_states_count = df.filter(col(\"State\").isNull()).count()\n",
    "    assert invalid_states_count != 0, \"There are invalid states in the State column\"\n",
    "    print(\"Test Case 2: Validate State Column - Passed\")\n",
    "\n",
    "# Test Case 3: Cast Price_per_product to DoubleType\n",
    "def test_cast_price_to_double(df):\n",
    "    assert df.schema[\"Price_per_product\"].dataType == DoubleType(), \"Price_per_product is not cast to DoubleType correctly\"\n",
    "    print(\"Test Case 3: Cast Price_per_product to DoubleType - Passed\")\n",
    "\n",
    "# Applying transformations and running tests\n",
    "df_product = col_name_standardization(df_product)\n",
    "test_col_name_standardization(df_product)\n",
    "\n",
    "# Validate State column and test\n",
    "df_product = df_product.withColumn(\"State\", validate_state_udf(lower(col(\"State\")))) \n",
    "test_validate_state_column(df_product)\n",
    "\n",
    "# Cast Price_per_product column to DoubleType and test\n",
    "df_product = cast_columns(df_product)  \n",
    "test_cast_price_to_double(df_product)\n",
    "\n",
    "# Display the final DataFrame\n",
    "#df_product.show(truncate=False)\n",
    "df_product.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7492400-2150-4615-89a1-eb5a3055a5fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Question 1. VALIDATING AND CREATING RAW TABLES FOR EACH SOURCE DATA.   \n",
    "a. Check for row count on raw dataframes.  \n",
    "b. validate column names.  \n",
    "c. if a and b pass, save as table and perform post validation check using sql (schema match, empty dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc5191c5-a81f-4248-b9fc-b0df9f2abc93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame validation passed successfully.\nTest for saving DataFrames as Delta tables passed successfully.\n"
     ]
    }
   ],
   "source": [
    "def validate_dataframes(df_order, df_customer, df_product):\n",
    "    # Check if DataFrames are not empty\n",
    "    assert df_order.count() > 0, \"Order DataFrame is empty.\"\n",
    "    assert df_customer.count() > 0, \"Customer DataFrame is empty.\"\n",
    "    assert df_product.count() > 0, \"Product DataFrame is empty.\"\n",
    "\n",
    "    # Check schema or other validations if necessary\n",
    "    # For example, validate required columns\n",
    "    required_order_columns = {'Order_ID', 'Product_ID', 'Customer_ID', 'Quantity', 'Order_Date'}\n",
    "    required_customer_columns = {'Customer_ID', 'Customer_Name', 'State', 'email'}\n",
    "    required_product_columns = {'Product_ID', 'Category', 'Sub_Category', 'Product_Name', 'Price_per_product'}\n",
    "\n",
    "    assert required_order_columns.issubset(set(df_order.columns)), \"Order DataFrame is missing required columns.\"\n",
    "    assert required_customer_columns.issubset(set(df_customer.columns)), \"Customer DataFrame is missing required columns.\"\n",
    "    assert required_product_columns.issubset(set(df_product.columns)), \"Product DataFrame is missing required columns.\"\n",
    "\n",
    "    print(\"DataFrame validation passed successfully.\")\n",
    "\n",
    "def test_save_as_table(df_order, df_customer, df_product):\n",
    "    try:\n",
    "        # Validate DataFrames before saving\n",
    "        validate_dataframes(df_order, df_customer, df_product)\n",
    "        \n",
    "        # Save DataFrames as Delta tables\n",
    "        df_order.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable(\"Order\")\n",
    "        df_customer.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable(\"Customer\")\n",
    "        df_product.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable(\"Product\")\n",
    "        \n",
    "        # Check if the tables are created\n",
    "        assert spark._jsparkSession.catalog().tableExists(\"Order\"), \"Order table was not created.\"\n",
    "        assert spark._jsparkSession.catalog().tableExists(\"Customer\"), \"Customer table was not created.\"\n",
    "        assert spark._jsparkSession.catalog().tableExists(\"Product\"), \"Product table was not created.\"\n",
    "        \n",
    "        # Check the count of the tables\n",
    "        assert spark.sql(\"SELECT COUNT(*) FROM Order\").first()[0] == df_order.count(), \"Row count mismatch for Order table.\"\n",
    "        assert spark.sql(\"SELECT COUNT(*) FROM Customer\").first()[0] == df_customer.count(), \"Row count mismatch for Customer table.\"\n",
    "        assert spark.sql(\"SELECT COUNT(*) FROM Product\").first()[0] == df_product.count(), \"Row count mismatch for Product table.\"\n",
    "        \n",
    "        # Optional: Drop the tables if needed\n",
    "        #spark.sql(\"DROP TABLE IF EXISTS Order\")\n",
    "        # spark.sql(\"DROP TABLE IF EXISTS Customer\")\n",
    "        #spark.sql(\"DROP TABLE IF EXISTS Product\")\n",
    "        \n",
    "        print(\"Test for saving DataFrames as Delta tables passed successfully.\")\n",
    "    \n",
    "    except AssertionError as e:\n",
    "        print(f\"AssertionError: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Run the test function\n",
    "test_save_as_table(df_order, df_customer, df_product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c7b496-c58e-4643-8efa-db287195e668",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Q2. Create an enriched table for customers and products "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "969b7397-fdca-402c-afe7-3aafa58d7c5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------------+----------+-------------------------------------------------+-----------+-------------+----------+--------------+-----------+-------+---------------+---------------+------------+----------------------------------------------------------------------------------------+--------------+-----------------+\n|Customer_ID|Customer_Name|email                     |phone     |address                                          |Segment    |Country      |City      |Customer_State|Postal_Code|Region |Product_ID     |Category       |Sub_Category|Product_Name                                                                            |Product_State |Price_per_product|\n+-----------+-------------+--------------------------+----------+-------------------------------------------------+-----------+-------------+----------+--------------+-----------+-------+---------------+---------------+------------+----------------------------------------------------------------------------------------+--------------+-----------------+\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |FUR-BO-10002213|Furniture      |Bookcases   |Sauder Forest Hills Library, Woodland Oak Finish                                        |california    |119.833          |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |FUR-BO-10002213|Furniture      |Bookcases   |DMI Eclipse Executive Suite Bookcases                                                   |new york      |400.784          |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |FUR-CH-10004086|Furniture      |Chairs      |Hon 4070 Series Pagoda Armless Upholstered Stacking Chairs                              |arizona       |233.25           |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-PA-10003441|Office Supplies|Paper       |Xerox 226                                                                               |california    |6.48             |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |FUR-TA-10000849|Furniture      |Tables      |Bevis Rectangular Conference Tables                                                     |colorado      |72.99            |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-AR-10001940|Office Supplies|Art         |Sanford Colorific Eraseable Coloring Pencils, 12 Count                                  |california    |3.28             |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-PA-10001583|Office Supplies|Paper       |1/4 Fold Party Design Invitations & White Envelopes, 24 8-1/2\" X 11\" Cards, 25 Env./Pack|delaware      |7.35             |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |TEC-PH-10003012|Technology     |Phones      |Nortel Meridian M3904 Professional Digital phone                                        |wisconsin     |153.99           |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-BI-10003527|Office Supplies|Binders     |Fellowes PB500 Electric Punch Plastic Comb Binding Machine with Manual Bind             |maryland      |1270.99          |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-AR-10003481|Office Supplies|Art         |Newell 348                                                                              |pennsylvania  |2.624            |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-BI-10000201|Office Supplies|Binders     |Avery Triangle Shaped Sheet Lifters, Black, 2/Pack                                      |florida       |0.738            |\n|PW-19240   |Pierre Wener |bettysullivan808@gmail.com|4215800902|001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462|Consumer   |United States|Louisville|Colorado      |80027      |West   |OFF-PA-10002259|Office Supplies|Paper       |Geographics Note Cards, Blank, White, 8 1/2\" x 11\"                                      |colorado      |8.75             |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|FUR-CH-10000229|Furniture      |Chairs      |Global Enterprise Series Seating High-Back Swivel/Tilt Chairs                           |texas         |189.686          |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|OFF-PA-10001289|Office Supplies|Paper       |White Computer Printout Paper by Universal                                              |illinois      |31.008           |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|OFF-ST-10003123|Office Supplies|Storage     |Fellowes Bases and Tops For Staxonsteel/High-Stak Systems                               |california    |33.29            |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|OFF-LA-10004559|Office Supplies|Labels      |Avery 49                                                                                |north carolina|2.304            |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|FUR-FU-10004164|Furniture      |Furnishings |Eldon 300 Class Desk Accessories, Black                                                 |washington    |4.95             |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|OFF-BI-10003291|Office Supplies|Binders     |Wilson Jones Leather-Like Binders with DublLock Round Rings                             |virginia      |8.73             |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|OFF-ST-10001505|Office Supplies|Storage     |Perma STOR-ALL Hanging File Box, 13 1/8\"W x 12 1/4\"D x 10 1/2\"H                         |california    |5.98             |\n|GH-14410   |Gary Hansen  |austindyer948@gmail.com   |5424150246|00347 Murphy Unions\\nAshleyton, IA 29814         |Home Office|United States|Chicago   |Illinois      |60653      |Central|FUR-FU-10003799|Furniture      |Furnishings |Seth Thomas 13 1/2\" Wall Clock                                                          |minnesota     |17.78            |\n+-----------+-------------+--------------------------+----------+-------------------------------------------------+-----------+-------------+----------+--------------+-----------+-------+---------------+---------------+------------+----------------------------------------------------------------------------------------+--------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_customer_products = spark.sql(\n",
    "    \"\"\"\n",
    "    Select \n",
    "    c.Customer_ID\n",
    "    ,c.Customer_Name\n",
    "    ,c.email\n",
    "    ,c.phone\n",
    "    ,c.address\n",
    "    ,c.Segment\n",
    "    ,c.Country\n",
    "    ,c.City\n",
    "    ,c.State as Customer_State\n",
    "    ,c.Postal_Code\n",
    "    ,c.Region\n",
    "    ,p.Product_ID\n",
    "    ,p.Category\n",
    "    ,p.Sub_Category\n",
    "    ,p.Product_Name\n",
    "    ,p.State as Product_State\n",
    "    ,p.Price_per_product\n",
    "    from customer as c\n",
    "    join `order` as o\n",
    "    join product as p\n",
    "    on c.Customer_ID = o.Customer_ID\n",
    "    and p.Product_ID = o.Product_ID\n",
    "    \"\"\")\n",
    "\n",
    "df_customer_products.show(truncate=False)\n",
    "\n",
    "def test_customer_products_table():\n",
    "    # Save dataframe as Delta table\n",
    "    df_customer_products.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable(\"enriched_customer_products\")\n",
    "\n",
    "    # Test case: check if the table exists\n",
    "    table_exists = spark._jsparkSession.catalog().tableExists(\"enriched_customer_products\")\n",
    "    assert table_exists\n",
    "\n",
    "    # Test case: check if the table has correct schema\n",
    "    expected_schema = [\n",
    "        'Customer_ID', 'Customer_Name', 'email', 'phone', 'address', 'Segment', 'Country', 'City', 'Customer_State',\n",
    "        'Postal_Code', 'Region', 'Product_ID', 'Category', 'Sub_Category', 'Product_Name', 'Product_State',\n",
    "        'Price_per_product'\n",
    "    ]\n",
    "    actual_schema = [field.name for field in spark.catalog.listColumns(\"enriched_customer_products\")]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the table has data\n",
    "    row_count = spark.sql(\"SELECT COUNT(*) FROM enriched_customer_products\").collect()[0][0]\n",
    "    assert row_count > 0\n",
    "\n",
    "test_customer_products_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26169765-9734-4321-b74c-0a6334a04e68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Q3. Create an enriched table which has order information, Profit rounded to 2 decimal places, Customer name and country, Product category and sub category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90fb1fb9-bc79-4a02-bb45-4f0c05bc0494",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Testing and writing enriched table. Running validation checks such as - schema correctness, count of records similar to the Q1 tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "925ef4b5-7b59-43d5-b8b9-5029cbf8ee84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+---------------+------------+------+\n|Customer_Name     |Country      |Category       |Sub_Category|Profit|\n+------------------+-------------+---------------+------------+------+\n|Jay Kimmel        |United States|Furniture      |Chairs      |63.69 |\n|Jay Kimmel        |United States|Furniture      |Chairs      |63.69 |\n|Bil Donatelli     |United States|Technology     |Accessories |102.19|\n|Laurel Beltran    |United States|Office Supplies|Binders     |-14.92|\n|Karl Braun        |United States|Office Supplies|Paper       |5.64  |\n|Denny Ordway      |United States|Technology     |Accessories |-3.0  |\n|Cassandra Brandow |United States|Office Supplies|Binders     |38.38 |\n|Sally Matthias    |United States|Office Supplies|Paper       |5.23  |\n|Rick Duston       |United States|Furniture      |Furnishings |5.19  |\n|Justin MacKendrick|United States|Office Supplies|Storage     |214.0 |\n|Scot Coram        |United States|Office Supplies|Storage     |4.18  |\n|Bil Overfelt      |United States|Office Supplies|Fasteners   |2.8   |\n|Bil Donatelli     |United States|Office Supplies|Binders     |41.12 |\n|Adrian Barton     |United States|Office Supplies|Binders     |-13.94|\n|Nra Paige         |United States|Technology     |Accessories |4.0   |\n|Karen Danels      |United States|Furniture      |Furnishings |10.0  |\n|Barry Franz       |United States|Furniture      |Furnishings |-378.4|\n|Laurel Elliston   |United States|Office Supplies|Binders     |-30.56|\n|Ted Trevino       |United States|Office Supplies|Storage     |10.35 |\n|Cthy Armstrong    |United States|Office Supplies|Binders     |47.0  |\n+------------------+-------------+---------------+------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_enriched_all = spark.sql(\n",
    "    \"\"\"\n",
    "    Select \n",
    "    c.Customer_Name\n",
    "    ,c.Country\n",
    "    ,p.Category\n",
    "    ,p.Sub_Category\n",
    "    ,Round(o.Profit, 2) as Profit\n",
    "    from `order` as o\n",
    "    join customer as c\n",
    "    join product as p\n",
    "    on c.Customer_ID = o.Customer_ID\n",
    "    and p.Product_ID = o.Product_ID\n",
    "    \"\"\")\n",
    "    \n",
    "df_enriched_all.show(truncate=False)\n",
    "\n",
    "def test_enriched_order_customer_product_table():\n",
    "    # Save dataframe as Delta table\n",
    "    df_enriched_all.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable(\"enriched_order_customer_product\")\n",
    "\n",
    "    # Test case: check if the table exists\n",
    "    table_exists = spark._jsparkSession.catalog().tableExists(\"enriched_order_customer_product\")\n",
    "    assert table_exists\n",
    "\n",
    "    # Test case: check if the table has correct schema\n",
    "    expected_schema = [\"Customer_Name\", \"Country\", \"Category\", \"Sub_Category\", \"Profit\",]\n",
    "    actual_schema = [field.name for field in spark.catalog.listColumns(\"enriched_order_customer_product\")]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the table has data\n",
    "    row_count = spark.sql(\"SELECT COUNT(*) FROM enriched_order_customer_product\").collect()[0][0]\n",
    "    assert row_count > 0\n",
    "\n",
    "test_enriched_order_customer_product_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "117a70d3-6d05-410e-9fbe-7d22c912abeb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Q4. Create an aggregate table that shows profit by Year, Product Category, Product Sub Category ,Customer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bc14e7a-25d2-417f-a5f5-00a267050c27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Testing and writing table. Running validation checks such as - schema correctness, count of records similar to Q2 tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a53a8fef-8db1-4818-813e-aff5e3e6f3d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+--------------------+----+------------------+\n|Category       |Sub_Category|Customer_ID|Customer_Name       |Year|Total_Profit      |\n+---------------+------------+-----------+--------------------+----+------------------+\n|Office Supplies|Supplies    |RD-19810   |Ross DeVincentis    |2017|10.518            |\n|Furniture      |Chairs      |DJ-13630   |Doug Jacobs         |2017|139.5702          |\n|Office Supplies|Binders     |EG-13900   |Emily Grady         |2017|-22.8956          |\n|Technology     |Phones      |BN-11470   |Brad Norvell        |2017|4.9616            |\n|Office Supplies|Labels      |AG-10495   |Andrew Gjertsen     |2017|9.0               |\n|Furniture      |Bookcases   |CS-12355   |Christine Sundaresam|2017|9.0882            |\n|Office Supplies|Storage     |PJ-19015   |Pauline Johnson     |2017|0.0               |\n|Technology     |Phones      |DM-13015   |Darrin Martin       |2017|51.4975           |\n|Technology     |Phones      |LL-16840   |Lauren Leatherbury  |2017|8.4564            |\n|Technology     |Machines    |MW-18235   |Mitch Willingham    |2017|216.0             |\n|Technology     |Accessories |SF-20965   |Sylvia Foulston     |2017|3.0134            |\n|Technology     |Phones      |BD-11500   |Bradley Drucker     |2017|764.3818          |\n|Technology     |Accessories |BH-11710   |Brosina Hoffman     |2017|22.4988           |\n|Office Supplies|Art         |KH-16630   |Ken Heidel          |2017|0.5004            |\n|Technology     |Accessories |CC-12430   |Chuck Clark         |2017|134.0             |\n|Office Supplies|Binders     |PK-19075   |Pete Kriz           |2017|1479.6914000000002|\n|Office Supplies|Paper       |FM-14290   |Frank Merwin        |2017|63.9302           |\n|Office Supplies|Art         |GZ-14545   |George Zrebassa     |2017|4.116             |\n|Technology     |Phones      |MG-17650   |Matthew Grinstein   |2017|-53.9482          |\n|Furniture      |Furnishings |CC-12220   |Chris Cortes        |2017|23.7864           |\n+---------------+------------+-----------+--------------------+----+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_profit_grouping = spark.sql(\"\"\" \n",
    "                  Select p.Category, p.Sub_Category, c.Customer_ID, c.Customer_Name\n",
    "                  ,year(o.Order_Date) as Year\n",
    "                  ,sum(Profit) as Total_Profit\n",
    "                  from `order` as o\n",
    "                  join product as p\n",
    "                  join customer as c\n",
    "                  on c.Customer_ID = o.Customer_ID and p.Product_ID = o.Product_ID \n",
    "                  group by p.Category, p.Sub_Category, c.Customer_ID, c.Customer_Name, year(o.Order_Date)\n",
    "                  order by Year desc\n",
    "                  \"\"\")\n",
    "\n",
    "df_profit_grouping.show(truncate=False)\n",
    "\n",
    "def test_profit_grouping():\n",
    "    # Save dataframe as Delta table\n",
    "    df_profit_grouping.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable(\"profit_by_year_product_customer\")\n",
    "\n",
    "    # Test case: check if the table exists\n",
    "    table_exists = spark._jsparkSession.catalog().tableExists(\"profit_by_year_product_customer\")\n",
    "    assert table_exists\n",
    "\n",
    "    # Test case: check if the table has correct schema\n",
    "    expected_schema = [\"Category\", \"Sub_Category\", \"Customer_ID\", \"Customer_Name\", \"Year\", \"Total_Profit\"]\n",
    "    actual_schema = [field.name for field in spark.catalog.listColumns(\"profit_by_year_product_customer\")]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the table has data\n",
    "    row_count = spark.sql(\"SELECT COUNT(*) FROM profit_by_year_product_customer\").collect()[0][0]\n",
    "    assert row_count > 0\n",
    "\n",
    "test_profit_grouping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0116aa-3131-4bd8-ae53-39227355cd56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Q5 A. Using SQL output the following aggregates Profit by Year.  \n",
    "Tests for 5a-5d: check for schema correctness and empty tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "519bc4b2-5b43-438b-ab4b-a0a56a5ad725",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n|year|yearly_profit     |\n+----+------------------+\n|2014|40975.45720000002 |\n|2015|65706.34270000015 |\n|2016|68161.4049        |\n|2017|127175.11319999972|\n+----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# SQL Query \n",
    "df_profit_by_year = spark.sql(\"\"\"\n",
    "      Select year,\n",
    "      sum(Total_Profit) as yearly_profit\n",
    "\n",
    "      from profit_by_year_product_customer\n",
    "      group by Year\n",
    "      order by Year\n",
    "      \"\"\")\n",
    "\n",
    "df_profit_by_year.show(truncate=False)\n",
    "\n",
    "def test_profit_by_year():\n",
    "\n",
    "    # Test case: check if the dataframe has correct schema\n",
    "    expected_schema = [\"year\", \"yearly_profit\"]\n",
    "    actual_schema = [field for field in df_profit_by_year.columns]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the dataframe has data\n",
    "    assert df_profit_by_year.select('year').distinct().count() == spark.sql(\"Select count(distinct year(Order_Date)) from order\").collect()[0][0]\n",
    "\n",
    "test_profit_by_year()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a9163f6-0894-4a11-a0bb-5afccbb6ee21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Q5 B. Using SQL output the following aggregates Profit by Year + Product Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c0e092-5469-4a58-9e9e-85e731b4e16a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+--------------------------+\n|Year|Product_Category|total_profit_by_year_prcat|\n+----+----------------+--------------------------+\n|2014|Technology      |23486.1854                |\n|2014|Furniture       |-5174.653799999999        |\n|2014|Office Supplies |22663.925599999995        |\n|2015|Office Supplies |25490.433699999998        |\n|2015|Furniture       |3392.157399999999         |\n|2015|Technology      |36823.75160000001         |\n|2016|Technology      |24437.399600000022        |\n|2016|Furniture       |7750.212199999998         |\n|2016|Office Supplies |35973.79309999996         |\n|2017|Office Supplies |45330.5905                |\n|2017|Technology      |78482.83110000004         |\n|2017|Furniture       |3361.6916                 |\n+----+----------------+--------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# SQL Query\n",
    "df_profit_by_year_prcat = spark.sql(\n",
    "  \"\"\"\n",
    "  Select Year, Category as Product_Category,\n",
    "  sum(Total_Profit) as total_profit_by_year_prcat\n",
    "  from profit_by_year_product_customer\n",
    "  group by Year, Product_Category\n",
    "  order by Year\n",
    "  \"\"\")\n",
    "\n",
    "df_profit_by_year_prcat.show(truncate=False)\n",
    "\n",
    "def test_profit_by_year_cat():\n",
    "\n",
    "    # Test case: check if the dataframe has correct schema\n",
    "    expected_schema = [\"Year\", \"Product_Category\", \"total_profit_by_year_prcat\",]\n",
    "    actual_schema = [field for field in df_profit_by_year_prcat.columns]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the dataframe has data\n",
    "    assert df_profit_by_year_prcat.select('Year','Product_Category').distinct().count() == spark.sql(\"Select count(distinct Year, Category) from profit_by_year_product_customer\").collect()[0][0]\n",
    "\n",
    "test_profit_by_year_cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2c0914-e4be-4c8d-b220-c6e6c8fb7945",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Q5 C. Using SQL output the following aggregates Profit by Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8b68d7-cdd8-4d8d-8702-152659a8f139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------------------+\n|Customer_ID|Customer_Name       |total_profit_by_customer|\n+-----------+--------------------+------------------------+\n|AA-10315   |Alex Avila          |-273.4089000000001      |\n|AA-10375   |Allen Armold        |277.3824                |\n|AA-10480   |Andrew Allen        |445.9694                |\n|AA-10645   |Anna Andreadi       |807.8329                |\n|AB-10015   |Aaron Bergman       |129.6821                |\n|AB-10060   |Adam Bellavance     |2047.8491000000001      |\n|AB-10105   |Adrian Barton       |5483.749                |\n|AB-10150   |Aimee Bixby         |320.68149999999997      |\n|AB-10165   |Alan Barnes         |215.36410000000004      |\n|AB-10255   |Alejandro Ballentine|264.5675                |\n|AB-10600   |Ann Blume           |-275.286                |\n|AC-10420   |Alyssa Crouse       |-62.13419999999999      |\n|AC-10450   |Amy Cox             |1730.0927000000001      |\n|AC-10615   |Ann Chong           |298.61330000000004      |\n|AC-10660   |Anna Chung          |-28.700399999999995     |\n|AD-10180   |Alan Dominguez      |1869.9294               |\n|AF-10870   |Art Ferguson        |317.63360000000006      |\n|AF-10885   |Art Foster          |-163.1175               |\n|AG-10270   |Alejandro Grove     |732.7399                |\n|AG-10300   |Aleksandra Gannaway |59.288399999999996      |\n+-----------+--------------------+------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Run SQL Query\n",
    "df_profit_by_customer = spark.sql(\n",
    "    \"\"\"\n",
    "    Select Customer_ID, Customer_Name,\n",
    "    sum(Total_Profit) as total_profit_by_customer\n",
    "    from profit_by_year_product_customer\n",
    "    group by Customer_ID, Customer_Name\n",
    "    order by Customer_ID\n",
    "    \"\"\")\n",
    "\n",
    "df_profit_by_customer.show(truncate=False)\n",
    "\n",
    "def test_profit_by_customer():\n",
    "\n",
    "    # Test case: check if the dataframe has correct schema\n",
    "    expected_schema = [\"Customer_ID\", \"Customer_Name\", \"total_profit_by_customer\"]\n",
    "    actual_schema = [field for field in df_profit_by_customer.columns]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the dataframe has data\n",
    "    assert df_profit_by_customer.select('Customer_ID').distinct().count() == spark.sql(\"Select count(distinct Customer_ID) from profit_by_year_product_customer\").collect()[0][0]\n",
    "\n",
    "test_profit_by_customer()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85fdc64b-5c6b-41c5-92c5-33d9a00eec19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Q5 D. Using SQL output the following aggregates Profit by Customer + year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e47c5794-5c0c-41b8-8c70-e3b57ad37a7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---------------+-----------------------------+\n|Year|Customer_ID|Customer_Name  |total_profit_by_year_customer|\n+----+-----------+---------------+-----------------------------+\n|2014|PG-18820   |Patrick Gardner|54.455200000000005           |\n|2014|DR-12940   |Daniel Raglin  |22.6782                      |\n|2014|BT-11530   |Bradley Talbott|66.77120000000001            |\n|2014|JL-15130   |Jack Lebron    |9.8672                       |\n|2014|TS-21205   |Thomas Seio    |471.8072                     |\n|2014|RA-19945   |Ryan Akin      |-436.74309999999997          |\n|2014|MC-17425   |Mark Cousins   |134.9955                     |\n|2014|JD-16015   |Joy Daniels    |-10.331700000000001          |\n|2014|MH-17440   |Mark Haberlin  |48.8491                      |\n|2014|TC-21475   |Tony Chapman   |-16.467                      |\n|2014|EH-13990   |Erica Hackney  |54.09199999999999            |\n|2014|AC-10660   |Anna Chung     |-4.9704                      |\n|2014|JF-15490   |Jeremy Farry   |35.3916                      |\n|2014|TH-21550   |Tracy Hopkins  |26.6304                      |\n|2014|TN-21040   |Tanja Norvell  |21.2518                      |\n|2014|RA-19285   |Ralph Arnett   |0.0                          |\n|2014|TT-21070   |Ted Trevino    |121.39469999999999           |\n|2014|GZ-14470   |Gary Zandusky  |50.4306                      |\n|2014|GH-14425   |Gary Hwang     |1070.5222                    |\n|2014|MY-18295   |Muhammed YeDwab|-103.13879999999999          |\n+----+-----------+---------------+-----------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    " # Run SQL Query\n",
    "df_profit_by_year_customer = spark.sql(\n",
    "    \"\"\"\n",
    "    Select Year, Customer_ID, Customer_Name,\n",
    "    sum(Total_Profit) as total_profit_by_year_customer\n",
    "    from profit_by_year_product_customer\n",
    "    group by\n",
    "    Year,Customer_ID,Customer_Name\n",
    "    order by Year\n",
    "    \"\"\")\n",
    "  \n",
    "df_profit_by_year_customer.show(truncate=False)\n",
    "\n",
    "def test_profit_by_year_customer():\n",
    "\n",
    "    # Test case: check if the dataframe has correct schema\n",
    "    expected_schema = [ \"Year\", \"Customer_ID\", \"Customer_Name\", \"total_profit_by_year_customer\"]\n",
    "    actual_schema = [field for field in df_profit_by_year_customer.columns]\n",
    "    assert actual_schema == expected_schema\n",
    "\n",
    "    # Test case: check if the dataframe has data\n",
    "    assert df_profit_by_year_customer.select('Year', 'Customer_ID').distinct().count() == spark.sql(\"Select count(distinct Year, Customer_ID) from profit_by_year_product_customer\").collect()[0][0]\n",
    "\n",
    "test_profit_by_year_customer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c85f1ed2-e9e0-46f1-be90-fedfda4fdc74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PEI Assessment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
